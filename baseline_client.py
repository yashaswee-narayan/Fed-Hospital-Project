# -*- coding: utf-8 -*-
"""baseline_client

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xgOAO0T0QKxKJM35LKOqAnkQXUakGHgU
"""

# baseline_client.ipynb
import numpy as np
import torch, torch.nn as nn, torch.optim as optim

class TabularMLP(nn.Module):
    def __init__(self, in_features, hidden=128, num_classes=2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_features, hidden),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden, hidden//2),
            nn.ReLU(),
            nn.Linear(hidden//2, num_classes)
        )
    def forward(self, x):
        return self.net(x)

def train_local_mlp(X_local, y_local, device='cpu', epochs=4, lr=1e-3, global_state=None, mu=0.0, batch_size=32):
    """
    Train MLP on local data.
    Returns: state_dict (numpy arrays), n_samples, avg_loss
    """
    device = torch.device(device)
    X_t = torch.tensor(X_local, dtype=torch.float32)
    y_t = torch.tensor(y_local, dtype=torch.long)
    ds = torch.utils.data.TensorDataset(X_t, y_t)
    loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)

    model = TabularMLP(X_local.shape[1]).to(device)
    if global_state is not None:
        # load global_state: dict key->numpy array
        sd = {k: torch.tensor(v).to(device) for k,v in global_state.items()}
        model.load_state_dict(sd)

    opt = optim.Adam(model.parameters(), lr=lr)
    crit = nn.CrossEntropyLoss()
    total_loss = 0.0; total_samples = 0
    for ep in range(epochs):
        for xb, yb in loader:
            xb, yb = xb.to(device), yb.to(device)
            opt.zero_grad()
            out = model(xb)
            loss = crit(out, yb)
            # FedProx proximal term
            if global_state is not None and mu > 0:
                prox = 0.0
                for name, p in model.named_parameters():
                    g = torch.tensor(global_state[name]).to(device)
                    prox += (p - g).norm()**2
                loss = loss + (mu/2.0) * prox
            loss.backward()
            opt.step()
            total_loss += loss.item() * xb.size(0)
            total_samples += xb.size(0)
    avg_loss = total_loss / max(1, total_samples)
    state = {k: v.cpu().detach().numpy() for k,v in model.state_dict().items()}
    return state, int(total_samples), float(avg_loss)

def eval_local_mlp(state_dict, X_eval, y_eval):
    model = TabularMLP(X_eval.shape[1])
    import torch
    model.load_state_dict({k: torch.tensor(v) for k,v in state_dict.items()})
    model.eval()
    with torch.no_grad():
        out = model(torch.tensor(X_eval, dtype=torch.float32))
        preds = out.numpy().argmax(axis=1)
    from sklearn.metrics import accuracy_score
    return float(accuracy_score(y_eval, preds))

